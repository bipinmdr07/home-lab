services:
  immich-server:
    container_name: immich-server
    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}
    user: "1000:1000"
    # extends:
    #   file: hwaccel.transcoding.yml
    #   service: cpu # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding
    volumes:
      - ${IMMICH_UPLOAD_PATH}/:/data
      - /etc/localtime:/etc/localtime:ro
      - shared_data:/mnt/shared
    devices:
      - /dev/dri:/dev/dri # For hardware Acceleration
    environment:
      - TZ=${TZ}
      - IMMICH_VERSION=release
      - DB_HOSTNAME=postgres
      - DB_DATABASE_NAME=immich
      - DB_USERNAME=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_STORAGE_TYPE=HDD # Options are HDD or SSD
    ports:
      - '2283:2283'
    depends_on:
      - redis
      - postgres
      - immich-machine-learning
    restart: always
    healthcheck:
      disable: false
    networks:
      - database
      - default
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.immich.rule=Host(`immich.${DOMAIN:-orangepi2.box}`)"
      - "traefik.http.routers.immich.entrypoints=web"
      - "traefik.http.services.immich.loadbalancer.server.port=2283"
      - "traefik.docker.network=homelab"
      - homepage.group=Media
      - homepage.name=immich
      - homepage.icon=si-immich
      - homepage.href=http://immich.${DOMAIN:-orangepi2.box}
      - homepage.widget.type=immich
      - homepage.widget.url=http://immich.${DOMAIN:-orangepi2.box}
      - homepage.widget.key=${IMMICH_API_KEY}
      - homepage.widget.version=2


  immich-machine-learning:
    container_name: immich-machine-learning
    # For hardware acceleration, add one of -[armnn, cuda, rocm, openvino, rknn] to the image tag.
    # Example tag: ${IMMICH_VERSION:-release}-cuda
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}
    # extends: # uncomment this section for hardware acceleration - see https://immich.app/docs/features/ml-hardware-acceleration
    #   file: hwaccel.ml.yml
    #   service: cpu # set to one of [armnn, cuda, rocm, openvino, openvino-wsl, rknn] for accelerated inference - use the `-wsl` version for WSL2 where applicable
    environment:
      # Ensure it binds to all interfaces
      - MACHINE_LEARNING_HOST=0.0.0.0
      - MACHINE_LEARNING_PORT=3003
    ports:
      # Optional: expose port for debugging
      - "3003:3003"
    volumes:
      - model-cache:/cache
        # env_file:
        # - .env
    restart: always
    networks:
      - default
    healthcheck:
      disable: false

